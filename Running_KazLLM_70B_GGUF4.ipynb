{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1a7ab4bd41d47f5b7e4d5002792d591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b4ba0135955473e91d7f0d7bd1a359f",
              "IPY_MODEL_2dce35a2ed8547e5a9b01bbe5cf4fc19",
              "IPY_MODEL_1553057461f34d53a2169748bdf1ee8b"
            ],
            "layout": "IPY_MODEL_39a3bab6631249d19d836ed8412616d4"
          }
        },
        "6b4ba0135955473e91d7f0d7bd1a359f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b298001381d4666875cf5cd8759f0e5",
            "placeholder": "​",
            "style": "IPY_MODEL_426ecc8299f94545af908bd3c9d97d0b",
            "value": "(…)htune_cabinet_28112024_18000-Q4_K_M.gguf: 100%"
          }
        },
        "2dce35a2ed8547e5a9b01bbe5cf4fc19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bcad8407e6d4d79943e44523c16b20e",
            "max": 42520397824,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3b725d92be24b43b67fd948d44ab575",
            "value": 42520397824
          }
        },
        "1553057461f34d53a2169748bdf1ee8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b59b738f54bb4803b5355852561c6074",
            "placeholder": "​",
            "style": "IPY_MODEL_871b1508e9894abd958d3194db2561f2",
            "value": " 42.5G/42.5G [16:53&lt;00:00, 42.3MB/s]"
          }
        },
        "39a3bab6631249d19d836ed8412616d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b298001381d4666875cf5cd8759f0e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "426ecc8299f94545af908bd3c9d97d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bcad8407e6d4d79943e44523c16b20e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3b725d92be24b43b67fd948d44ab575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b59b738f54bb4803b5355852561c6074": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "871b1508e9894abd958d3194db2561f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Launching KazLLM with 70B parameters and 4 bit quantized model using llama-cpp-python"
      ],
      "metadata": {
        "id": "lUHjK8OkDrb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To rusn the model i used google colab pro subscription with A100 GPU, 40GB vRAM, 230GB memory"
      ],
      "metadata": {
        "id": "0AQr6NbSc_Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the necessary libraries\n",
        "# Install llama-cpp-python for running GGUF models\n",
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "maCCG0HXy7Ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67b4aa0-11b7-42bb-ce7d-a4f34500fba8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.6.tar.gz (66.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.6-cp311-cp311-linux_x86_64.whl size=4069006 sha256=7643213e61af0c237dacd31996d541ed5d4b126dc13e31cd6892f4f9cb6feb57\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/96/d2/acfb576f7a58ef0580e2fec8096e5eefd17cc356017089337b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Download the 4-bit quantized KazLLM-70B model\n",
        "# Replace the URL below with the actual model's URL from Hugging Face or other sources\n",
        "!wget --header=\"Authorization: Bearer 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\" https://huggingface.co/issai/LLama-3.1-KazLLM-1.0-70B-GGUF4/resolve/main/Nemotron_70B_instruct_corex5_mcq_cleaned_old_torchtune_cabinet_28112024_18000-Q4_K_M.gguf -O /content/drive/MyDrive/LLM_models/Llama3-KazLLM-70B-4b-quantized.gguf"
      ],
      "metadata": {
        "id": "LNPKAWm0y698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9725701-937f-4b98-d622-a57d8d3db8f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-26 10:52:41--  https://huggingface.co/issai/LLama-3.1-KazLLM-1.0-70B-GGUF4/resolve/main/Nemotron_70B_instruct_corex5_mcq_cleaned_old_torchtune_cabinet_28112024_18000-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.160.143.99, 18.160.143.32, 18.160.143.75, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.160.143.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/6d/1f/6d1f8807693d43399c7574c10aa17a9a8fa4cb42a1123e81be20b0ddbfdde3b0/00932566d6f745533eb4285db7cf58102dc1ea111e4ee04a46548be2f713f13f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Nemotron_70B_instruct_corex5_mcq_cleaned_old_torchtune_cabinet_28112024_18000-Q4_K_M.gguf%3B+filename%3D%22Nemotron_70B_instruct_corex5_mcq_cleaned_old_torchtune_cabinet_28112024_18000-Q4_K_M.gguf%22%3B&Expires=1737892361&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzg5MjM2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzZkLzFmLzZkMWY4ODA3NjkzZDQzMzk5Yzc1NzRjMTBhYTE3YTlhOGZhNGNiNDJhMTEyM2U4MWJlMjBiMGRkYmZkZGUzYjAvMDA5MzI1NjZkNmY3NDU1MzNlYjQyODVkYjdjZjU4MTAyZGMxZWExMTFlNGVlMDRhNDY1NDhiZTJmNzEzZjEzZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=KOuCMp6XynNuhUHAOS-7fVuL1HLiMw1HE4Ar%7EoxbvqLP-9unNQU4BJc9NfNvIk5hNzw-cL05t8Qm9%7EOwoAGiPwQ0ZbkCTPkeqcxs3a8jN998xyoFRa-9yz6jJWzoIj%7EtsEGng5gs9DSjVzpTjD%7EcqYk3FE1Tqlx%7ETTw74cDHbPUKkSSA-lJiOBQmn8wK-Aa4SeKVzCfPusYFUnJegqLuchvsoe3sLAGplioEip88kIP3NYwL4Geli2gwSHpRGQXtlqRbTz4mUPkaa%7E2I8PyHEqA-CbTLR21KpAyDPxjhpt4eCnQBnaY7FPKmRh9z2zei7HwAMtPe7US5AuW1QXZ9Hw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-01-26 10:52:41--  https://cdn-lfs-us-1.hf.co/repos/6d/1f/6d1f8807693d43399c7574c10aa17a9a8fa4cb42a1123e81be20b0ddbfdde3b0/00932566d6f745533eb4285db7cf58102dc1ea111e4ee04a46548be2f713f13f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Nemotron_70B_instruct_corex5_mcq_cleaned_old_torchtune_cabinet_28112024_18000-Q4_K_M.gguf%3B+filename%3D%22Nemotron_70B_instruct_corex5_mcq_cleaned_old_torchtune_cabinet_28112024_18000-Q4_K_M.gguf%22%3B&Expires=1737892361&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzg5MjM2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzZkLzFmLzZkMWY4ODA3NjkzZDQzMzk5Yzc1NzRjMTBhYTE3YTlhOGZhNGNiNDJhMTEyM2U4MWJlMjBiMGRkYmZkZGUzYjAvMDA5MzI1NjZkNmY3NDU1MzNlYjQyODVkYjdjZjU4MTAyZGMxZWExMTFlNGVlMDRhNDY1NDhiZTJmNzEzZjEzZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=KOuCMp6XynNuhUHAOS-7fVuL1HLiMw1HE4Ar%7EoxbvqLP-9unNQU4BJc9NfNvIk5hNzw-cL05t8Qm9%7EOwoAGiPwQ0ZbkCTPkeqcxs3a8jN998xyoFRa-9yz6jJWzoIj%7EtsEGng5gs9DSjVzpTjD%7EcqYk3FE1Tqlx%7ETTw74cDHbPUKkSSA-lJiOBQmn8wK-Aa4SeKVzCfPusYFUnJegqLuchvsoe3sLAGplioEip88kIP3NYwL4Geli2gwSHpRGQXtlqRbTz4mUPkaa%7E2I8PyHEqA-CbTLR21KpAyDPxjhpt4eCnQBnaY7FPKmRh9z2zei7HwAMtPe7US5AuW1QXZ9Hw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.170.152.28, 3.170.152.84, 3.170.152.55, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.170.152.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42520397824 (40G) [binary/octet-stream]\n",
            "Saving to: ‘/content/drive/MyDrive/LLM_models/Llama3-KazLLM-70B-4b-quantized.gguf’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>]  39.60G  40.1MB/s    in 16m 53s \n",
            "\n",
            "2025-01-26 11:09:34 (40.0 MB/s) - ‘/content/drive/MyDrive/LLM_models/Llama3-KazLLM-70B-4b-quantized.gguf’ saved [42520397824/42520397824]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#logging to HF via colab using my access token\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK5iPd_UeLt-",
        "outputId": "4aa5ade4-6d3e-435a-b0a6-d20dfb0c7754"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `for_colab` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `for_colab`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Import the llama-cpp library\n",
        "# llama-cpp-python allows running LLaMA-based models in Python\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Step 4: Initialize the model\n",
        "# Load the model and configure its performance parameters\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"issai/LLama-3.1-KazLLM-1.0-70B-GGUF4\", # model repo_id from HF\n",
        "\tfilename=\"Nemotron_70B_instruct_corex5_mcq_cleaned_old_torchtune_cabinet_28112024_18000-Q4_K_M.gguf\", # Path to the downloaded model\n",
        "\tn_threads=8,        # Number of cpu threads for processing\n",
        "\tn_ctx=2048,         # max context length (affects memory usage)\n",
        "\tn_gpu_layers=70,    # Load all 70 layers of the model on the GPU\n",
        "\tuse_mmap=True)       # Optimize memory mapping for efficient loading\n"
      ],
      "metadata": {
        "id": "mrnSWbvxy6vN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c1a7ab4bd41d47f5b7e4d5002792d591",
            "6b4ba0135955473e91d7f0d7bd1a359f",
            "2dce35a2ed8547e5a9b01bbe5cf4fc19",
            "1553057461f34d53a2169748bdf1ee8b",
            "39a3bab6631249d19d836ed8412616d4",
            "7b298001381d4666875cf5cd8759f0e5",
            "426ecc8299f94545af908bd3c9d97d0b",
            "6bcad8407e6d4d79943e44523c16b20e",
            "c3b725d92be24b43b67fd948d44ab575",
            "b59b738f54bb4803b5355852561c6074",
            "871b1508e9894abd958d3194db2561f2"
          ]
        },
        "outputId": "ae65d8ec-fb28-4159-83da-8147ab4cc8c3",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1a7ab4bd41d47f5b7e4d5002792d591",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)htune_cabinet_28112024_18000-Q4_K_M.gguf:   0%|          | 0.00/42.5G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 26 key-value pairs and 724 tensors from /root/.cache/huggingface/hub/models--issai--LLama-3.1-KazLLM-1.0-70B-GGUF4/snapshots/d16f00ea3af3f595c827a2043b70a0c95fd1b231/./Nemotron_70B_instruct_corex5_mcq_cleaned_old_torchtune_cabinet_28112024_18000-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Nemotron_70B_instruct_corex5_mcq_clea...\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 71B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 28672\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  162 tensors\n",
            "llama_model_loader: - type q4_K:  441 tensors\n",
            "llama_model_loader: - type q5_K:   40 tensors\n",
            "llama_model_loader: - type q6_K:   81 tensors\n",
            "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 8192\n",
            "llm_load_print_meta: n_layer          = 80\n",
            "llm_load_print_meta: n_head           = 64\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 28672\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 70B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 70.55 B\n",
            "llm_load_print_meta: model size       = 39.59 GiB (4.82 BPW) \n",
            "llm_load_print_meta: general.name     = Nemotron_70B_instruct_corex5_mcq_cleaned_old_torchtune_cabinet_28112024_18000\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 802 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "llm_load_tensors:   CPU_Mapped model buffer size = 40543.11 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 2048\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 48: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 49: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 50: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 51: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 52: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 53: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 54: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 55: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 56: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 57: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 58: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 59: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 60: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 61: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 62: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 63: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 64: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 65: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 66: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 67: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 68: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 69: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 70: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 71: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 72: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 73: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 74: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 75: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 76: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 77: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 78: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 79: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:        CPU KV buffer size =   640.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   324.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 2566\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.value_length': '128', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '64', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Nemotron_70B_instruct_corex5_mcq_cleaned_old_torchtune_cabinet_28112024_18000', 'general.type': 'model', 'general.size_label': '71B', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8', 'llama.attention.key_length': '128'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- set date_string = \"26 Jul 2024\" %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message + builtin tools #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if builtin_tools is defined or tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{%- if builtin_tools is defined %}\n",
            "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
            "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
            "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- endif %}\n",
            "                {%- endfor %}\n",
            "            {{- \")\" }}\n",
            "        {%- else  %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "            {{- '\"parameters\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- \"}\" }}\n",
            "        {%- endif %}\n",
            "        {%- if builtin_tools is defined %}\n",
            "            {#- This means we're in ipython mode #}\n",
            "            {{- \"<|eom_id|>\" }}\n",
            "        {%- else %}\n",
            "            {{- \"<|eot_id|>\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|eot_id|>\n",
            "Using chat bos_token: <|begin_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Test the model with a sample prompt\n",
        "# Use the create_chat_completion method to generate responses\n",
        "response = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Ты ассистент, отвечающий на казахском языке. Сгенерируй тест из 10 вопросов с 4 ответами на тему который был введен. Выдели правильный ответ при выводе.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Ғылым\"}\n",
        "    ],\n",
        "    max_tokens=1000,   # Limit the length of the response\n",
        "    temperature=0.7,   # Adjust the creativity of the response\n",
        "    top_p=0.9          # Probabilistic sampling for diverse output\n",
        ")"
      ],
      "metadata": {
        "id": "dVC2gVr_y6sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b164b9-3939-4e16-d75a-313b167e6ef9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   51424.64 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /   936 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time = 1031449.19 ms /  1006 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Print the generated response\n",
        "# Output the model's response for the given input\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "id": "Wexga77-y6qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1415de6a-75ae-4dbd-c682-f04ee68e3e44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ғылым туралы 10 сұрақтың тесті: 1. Төмендегі ғылымдардың қайсысы биологияға жатпайды? а) Ботаника ә) Зоология б) Астрономия г) Микробиология 2. Ғылыми әдістің негізгі қадамы қандай? а) Бақылау жасау б) Сұрақтар қою б) Гипотеза жасау г) Нәтижелерді талдау 3. Қай ғалым қозғалыс заңдарын ашты? а) Галилео Галилей б) Исаак Ньютон б) Альберт Эйнштейн г) Леонардо да Винчи 4. Бұл ғылымның қай саласы Жердің физикалық ерекшеліктерін зерттейді? а) Астрономия б) Геология в) Биология г) Физика 5. Ғылыми теорияға қандай келесі анықтама жатады? а) Бақыланатын фактілер мен деректермен расталған құбылыстың дәлелденген түсіндірмесі б) Ғылыми бақылау мен эксперимент негізінде дәлелденбеген құбылыстың болжамды түсіндірмесі в) Құбылыс туралы алғашқы болжам г) Ғылыми эксперименттер нәтижесінде алынған деректердің жиынтығы 6. Ғылыми дискурстың қай түрі эмпирикалық дәлелдер мен дәлелдерге негізделеді? а) Апостериорлық б) Априорлық в) Индуктивті г) Дедуктивті 7. Ағылшын физигі Р.Х. Фейнман деген кім? а) Кванттық механиканың дамуына өлшеусіз үлес қосқан б) Электромагнетизм заңдарын ашқан в) Күн жүйесіндегі планеталардың қозғалысын зерттеген г) Жер атмосферасының құрамын зерттеген 8. Қай ғылымның негізгі мақсаты – ағзалардың құрылысын, қызметін және қоршаған ортамен қарым-қатынасын зерттеу? а) Физика б) Химия в) Биология г) Геология 9. Қай ғалым электромагниттік сәулеленудің жылдамдығын ашты? а) Исаак Ньютон б) Альберт Эйнштейн в) Джеймс Клерк Максвелл г) Галилео Галилей 10. Қай ғалым элементтердің периодтық жүйесін жасаған? а) Иоганн Дальтон б) Антуан Лавуазье в) Дмитрий Менделеев г) Генри Мозли\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DaHbz2VCoVk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}